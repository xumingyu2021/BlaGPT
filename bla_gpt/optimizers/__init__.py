import importlib
from typing import List

import torch


def get_optimizer(
    optimizer_name: str,
    optimizer_params: dict,
    lr: float,
    model: torch.nn.Module = None,
    parameters: List = None,
) -> torch.optim.Optimizer:
    """Find, initialize and return a Torch optimizer.

    Args:
        optimizer_name (str): Optimizer name.
        optimizer_params (dict): Optimizer parameters.
        lr (float): Initial learning rate.
        model (torch.nn.Module): Model to pass to the optimizer.

    Returns:
        torch.optim.Optimizer: Functional optimizer.
    """
    if model is not None:
        parameters = model.parameters()

    if optimizer_name.lower() == "radam":
        module = importlib.import_module("optimizers.radam")
        optimizer = getattr(module, "RAdam")
    elif optimizer_name.lower() == "palm_soap":
        try:
            from heavyball import PaLMForeachSOAP
        except ImportError:
            raise ImportError(
                "To use PaLMForeachSOAP, please install the heavyball package."
            )

        module = PaLMForeachSOAP
    elif optimizer_name.lower() == "ademamix":
        module = importlib.import_module("optimizers.ademamix")
        optimizer = getattr(module, "AdEMAMix")
    elif optimizer_name.lower() == "adopt":
        module = importlib.import_module("optimizers.adopt")
        optimizer = getattr(module, "ADOPT")
        return optimizer(parameters, lr=lr, **optimizer_params, decoupled=True)
    elif optimizer_name.lower() == "adamw_indep":
        module = importlib.import_module("optimizers.adamw_indep_weight_decay")
        optimizer = getattr(module, "AdamW")
    elif optimizer_name.lower() == "c_adamw":
        module = importlib.import_module("optimizers.c_adamw")
        optimizer = getattr(module, "AdamW")
    elif optimizer_name.lower() == "demo":
        module = importlib.import_module("optimizers.demo")
        optimizer = getattr(module, "DeMo")
    elif optimizer_name.lower() == "adam_mini":
        try:
            from adam_mini import Adam_mini
        except ImportError:
            raise ImportError("To use Adam_mini, please install the adam-mini package.")

        optimizer = Adam_mini(
            named_parameters=model.named_parameters(), lr=lr, **optimizer_params
        )
        optimizer.wqk_names.add("kv_proj")
        optimizer.attn_proj_names.add("c_proj")

        return optimizer
    else:
        optimizer = getattr(torch.optim, optimizer_name)
    return optimizer(parameters, lr=lr, **optimizer_params)
